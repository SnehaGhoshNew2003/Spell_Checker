{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCRD7M2VXoUy"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import nltk\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.corpus import stopwords\n",
        "import ast\n",
        "import torch"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbSM-aH_R8br"
      },
      "outputs": [],
      "source": [
        "!pip install datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UIAPoQIQSb3b"
      },
      "outputs": [],
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset = load_dataset(\"jfleg\")\n",
        "print(dataset[\"validation\"][0])\n",
        "print(dataset[\"validation\"][1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YBrigYJJSbz-"
      },
      "outputs": [],
      "source": [
        "dataset[\"validation\"].to_csv(\"text_validation.csv\")\n",
        "if \"train\" in dataset:\n",
        "    dataset[\"train\"].to_csv(\"text_train.csv\")\n",
        "if \"test\" in dataset:\n",
        "    dataset[\"test\"].to_csv(\"text_test.csv\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4XEHW8aySbxn"
      },
      "outputs": [],
      "source": [
        "df = pd.read_csv(\"text_validation.csv\")\n",
        "print(df.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3peTt6BOaaCJ"
      },
      "outputs": [],
      "source": [
        "df"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "m6KOhi4vUd9K"
      },
      "source": [
        "# Corrections column preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "FbLpcVdKbXK2"
      },
      "outputs": [],
      "source": [
        "print(type(df['corrections'])) # checking the type of corrections column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7di_LCqb2Xq"
      },
      "outputs": [],
      "source": [
        "print(type(df[\"corrections\"][0])) #checking the type of elements in the corrections column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eEVrg4M0dAQn"
      },
      "outputs": [],
      "source": [
        "df['corrections'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qcroLcaLdQ1I"
      },
      "outputs": [],
      "source": [
        "df[\"corrections\"] = df[\"corrections\"].apply(lambda x: [sentence.replace(\"\\n\", \"\").strip(\"' \").strip()for sentence in ast.literal_eval(x)] if isinstance(x, str) else x) # removing the extra \"\\n\" and single quotes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EvM8K8LKdXqn"
      },
      "outputs": [],
      "source": [
        "df['corrections'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mUrJod2ZATpy"
      },
      "outputs": [],
      "source": [
        "print(type(df['corrections'][0])) # checking type after removing unwanted objects"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4Kew00WeChet"
      },
      "outputs": [],
      "source": [
        "def split_corrections(text):\n",
        "    if isinstance(text, list) and len(text) == 1: #checks that if the length is 1\n",
        "        text = text[0]\n",
        "    if isinstance(text, str):\n",
        "        return [sentence.strip() for sentence in text.split(\".\") if sentence.strip()] #splitting the sentence on '.'\n",
        "    return text\n",
        "\n",
        "df[\"corrections\"] = df[\"corrections\"].apply(split_corrections)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "imganSSyV7al"
      },
      "outputs": [],
      "source": [
        "df['corrections']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS9ie1EEb-aQ"
      },
      "outputs": [],
      "source": [
        "df[\"corrections\"] = df[\"corrections\"].apply(lambda x: ast.literal_eval(x) if isinstance(x, str) else x) # checks and converts into string if its not in str type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "d6susgyMZlie"
      },
      "outputs": [],
      "source": [
        "expanded_data = [] # creating a list of new data\n",
        "for _, row in df.iterrows(): # iter through each row\n",
        "    sentence = row[\"sentence\"]\n",
        "    for corrections in row[\"corrections\"]:\n",
        "        expanded_data.append({\"sentence\": sentence, \"corrections\": corrections}) # making a list of dictionaries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "p0OByn0_EccE"
      },
      "outputs": [],
      "source": [
        "print(expanded_data[4])\n",
        "print(expanded_data[5])\n",
        "print(expanded_data[6])\n",
        "print(expanded_data[7])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UBtFCoSHXXcC"
      },
      "outputs": [],
      "source": [
        "print(type(expanded_data))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Uz-QFUlRXgDx"
      },
      "outputs": [],
      "source": [
        "print(type(expanded_data[0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u1c5l4D5ZpFa"
      },
      "outputs": [],
      "source": [
        "df_expanded = pd.DataFrame(expanded_data) #make a dataframe from the list\n",
        "print(df_expanded)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zzC0QsYtcS6o"
      },
      "outputs": [],
      "source": [
        "df_expanded"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8KEnQliEatuW"
      },
      "outputs": [],
      "source": [
        "df_expanded['sentence'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bcD4jQgSaxQz"
      },
      "outputs": [],
      "source": [
        "df_expanded['corrections'][0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df['corrections'] = df['corrections'].str.lower()"
      ],
      "metadata": {
        "id": "--ezd4HupskI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZHoBCuALcVQZ"
      },
      "source": [
        "## Tokenizing the corrections column using transformer **Bert** tokenizer also lower cased the elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_jhf_xaYMya5"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "df_expanded['corrections'] = df_expanded['corrections'].astype(str).str.lower()\n",
        "df_expanded['tokenized_corrections'] = df_expanded['corrections'].apply(lambda text: tokenizer.tokenize(text))\n",
        "df_expanded['tokenized_corrections']=df_expanded['tokenized_corrections'].apply(lambda tokens: \"\".join(tokens).replace(\"‚ñÅ\", \" \").strip())\n",
        "print(df_expanded.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PPPIfEkBM6Os"
      },
      "outputs": [],
      "source": [
        "df_expanded['corrections'][0]"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "df_expanded['tokenized_corrections'][0]"
      ],
      "metadata": {
        "id": "6X3GDl3iuvj7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aDdxJJe_S2CA"
      },
      "outputs": [],
      "source": [
        "df_expanded['tokenized_corrections'] = df_expanded['tokenized_corrections'].apply(lambda x: x.split() if isinstance(x, str) else x) #changing the type from str to list"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MAOviOGRS8ps"
      },
      "outputs": [],
      "source": [
        "df_expanded['tokenized_corrections'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KhDHGCJldAp-"
      },
      "source": [
        "## Removing the stopwords from corrections column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "woekeVCSTO0S"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "df_expanded['filtered_corrected_tokens'] = df_expanded['tokenized_corrections'].apply(lambda tokens:[word for word in tokens if word not in stop_words])\n",
        "df_expanded.head()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "b9hmbGZvdIPL"
      },
      "source": [
        "## Stemming the correction column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qQ0pAuRLT8J-"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "df_expanded[\"stemmed_corrections\"] = df_expanded[\"filtered_corrected_tokens\"].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
        "print(df_expanded.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8qlD34RUxnP"
      },
      "source": [
        "# Sentence column preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i386AXwLVAqv"
      },
      "outputs": [],
      "source": [
        "print(type(df['sentence'][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lhADfeNeVGE0"
      },
      "outputs": [],
      "source": [
        "print(type(df['sentence']))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sg4LErwtVKgK"
      },
      "outputs": [],
      "source": [
        "df['sentence'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZM2ffD8xdPAy"
      },
      "source": [
        "## Tokenized the sentence column using transformer **Bert** tokenizer also lower cased the elements"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UaHXPjK9Sbu1"
      },
      "outputs": [],
      "source": [
        "from transformers import T5Tokenizer\n",
        "tokenizer = T5Tokenizer.from_pretrained(\"t5-base\")\n",
        "df_expanded['sentence'] = df_expanded['sentence'].astype(str).str.lower()\n",
        "df_expanded['tokenized_sentence'] = df_expanded['sentence'].apply(lambda text: tokenizer.tokenize(text))\n",
        "df_expanded['tokenized_sentence']=df_expanded['tokenized_sentence'].apply(lambda tokens: \"\".join(tokens).replace(\"‚ñÅ\", \" \").strip())\n",
        "print(df_expanded.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5Z5T70S7HTzs"
      },
      "outputs": [],
      "source": [
        "df_expanded['tokenized_sentence'][4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "17y9ckbWQcWV"
      },
      "outputs": [],
      "source": [
        "df_expanded['sentence'][4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gGzfNQJe6UxX"
      },
      "outputs": [],
      "source": [
        "print(df_expanded['sentence'][0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CYCtScH6LRc"
      },
      "outputs": [],
      "source": [
        "print(type(df_expanded[\"tokenized_sentence\"][0]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2gO1PxS_SGlK"
      },
      "outputs": [],
      "source": [
        "df_expanded['tokenized_sentence'] = df_expanded['tokenized_sentence'].apply(lambda x: x.split() if isinstance(x, str) else x) # converting elements to str type"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "cDKFCN40d01D"
      },
      "outputs": [],
      "source": [
        "df_expanded['tokenized_sentence'][0]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Eknh-vi1eOxn"
      },
      "source": [
        "## Removing the stopwords from the sentence column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gYyhPAo-BKjO"
      },
      "outputs": [],
      "source": [
        "nltk.download(\"stopwords\")\n",
        "stop_words = set(stopwords.words(\"english\"))\n",
        "df_expanded['filtered_sentence_tokens'] = df_expanded['tokenized_sentence'].apply(lambda tokens:[word for word in tokens if word not in stop_words])\n",
        "df_expanded.head()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Fk0x3u6oKuT7"
      },
      "outputs": [],
      "source": [
        "df_expanded['filtered_corrected_tokens'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uAxlC-_LTjtL"
      },
      "outputs": [],
      "source": [
        "df_expanded['filtered_sentence_tokens'][0]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fgq810CmDq6Z"
      },
      "outputs": [],
      "source": [
        "df_expanded[\"sentence\"][4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yid0tdY8DjPe"
      },
      "outputs": [],
      "source": [
        "df_expanded['tokenized_sentence'][4]"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5l9o_AyKebaR"
      },
      "source": [
        "## Stemmed the sentence column"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gl0bcm_nSbpM"
      },
      "outputs": [],
      "source": [
        "stemmer = PorterStemmer()\n",
        "df_expanded[\"stemmed_sentence\"] = df_expanded[\"filtered_sentence_tokens\"].apply(lambda tokens: [stemmer.stem(token) for token in tokens])\n",
        "print(df_expanded.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H3_HfOUyAKsM"
      },
      "outputs": [],
      "source": [
        "df_expanded['stemmed_sentence'][4]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "fGltEnDzE8qR"
      },
      "outputs": [],
      "source": [
        "df_expanded = df_expanded.dropna(subset=[\"sentence\", \"tokenized_sentence\", \"corrections\",\"tokenized_corrections\"]) # dropped the NaN values"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Final Dataset"
      ],
      "metadata": {
        "id": "Pn0W0ouhdDOz"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_2Djd7XmXk_5"
      },
      "outputs": [],
      "source": [
        "df_expanded.to_csv(\"final_df.csv\", index=False)\n",
        "df_expanded.head() # final DataFrame after preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AoUJalyfAKhy"
      },
      "outputs": [],
      "source": [
        "df_expanded.columns"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "final_df = pd.read_csv(\"final_df.csv\", usecols=[\"sentence\",\"tokenized_sentence\", \"correction\",\"tokenized_corrections\"])"
      ],
      "metadata": {
        "id": "GoJBUTTXcecl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LKJ11hBGTZgy"
      },
      "source": [
        "## Further preprocessed the texts for training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nw4LXdD6Ueb-"
      },
      "outputs": [],
      "source": [
        "# Convert tokenized words to token IDs (already done in preprocessing)\n",
        "sentence_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in final_df[\"tokenized_sentence\"]]\n",
        "corrections_ids = [tokenizer.convert_tokens_to_ids(tokens) for tokens in final_df[\"tokenized_corrections\"]]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-jAB7WgDUgel"
      },
      "outputs": [],
      "source": [
        "# Manually pad sequences with 0\n",
        "max_len = max(max(len(seq) for seq in sentence_ids), max(len(seq) for seq in corrections_ids))\n",
        "sentence_ids = [seq + [0] * (max_len - len(seq)) for seq in sentence_ids]\n",
        "corrections_ids = [seq + [0] * (max_len - len(seq)) for seq in corrections_ids]\n",
        "print(\"Example sentence sequence:\", sentence_ids[0])\n",
        "print(\"Example corrections sequence:\", corrections_ids[0])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bzkgd6iuOLhG"
      },
      "outputs": [],
      "source": [
        "# converting into PyTorch Tensors\n",
        "sentence_ids = torch.tensor(sentence_ids, dtype=torch.long)\n",
        "corrections_ids = torch.tensor(corrections_ids, dtype=torch.long)\n",
        "print(\"Sentence IDs shape:\", sentence_ids.shape)\n",
        "print(\"Corrections IDs shape:\", corrections_ids.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hizwSbYoVGvP"
      },
      "source": [
        "## Demo code (Needs more understanding)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R08NzEjcOLcQ"
      },
      "outputs": [],
      "source": [
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "class SpellCorrectionDataset(Dataset):\n",
        "    def __init__(self, sentence_ids, corrections_ids):\n",
        "        self.sentence_ids = sentence_ids\n",
        "        self.corrections_ids = corrections_ids\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.sentence_ids)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        return {\n",
        "            \"sentence_ids\": self.sentence_ids[idx],\n",
        "            \"corrections_ids\": self.corrections_ids[idx]\n",
        "        }\n",
        "\n",
        "# Create dataset\n",
        "dataset = SpellCorrectionDataset(sentence_ids, corrections_ids)\n",
        "\n",
        "# Create DataLoader for batching\n",
        "batch_size = 16  # Adjust based on memory availability\n",
        "train_loader = DataLoader(dataset, batch_size=batch_size, shuffle=True)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7Mb6yCp3OLZ5"
      },
      "outputs": [],
      "source": [
        "from transformers import AutoModelForSeq2SeqLM\n",
        "\n",
        "# Load pre-trained model\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\"t5-base\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "y5AetWEFOLYH"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from transformers import AdamW\n",
        "\n",
        "# Define loss function\n",
        "criterion = torch.nn.CrossEntropyLoss(ignore_index=0)  # Ignore padding tokens\n",
        "\n",
        "# Define optimizer\n",
        "optimizer = AdamW(model.parameters(), lr=5e-5)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZydbF5hyOLVo"
      },
      "outputs": [],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "\n",
        "epochs = 3  # Adjust as needed\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    model.train()\n",
        "    total_loss = 0\n",
        "\n",
        "    for batch in train_loader:\n",
        "        sentence_ids = batch[\"sentence_ids\"].to(device)\n",
        "        corrections_ids = batch[\"corrections_ids\"].to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        outputs = model(input_ids=sentence_ids, labels=corrections_ids)\n",
        "        loss = outputs.loss\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(train_loader)\n",
        "    print(f\"Epoch {epoch+1}/{epochs}, Loss: {avg_loss:.4f}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "r82TfghSOLTS"
      },
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "with torch.no_grad():\n",
        "    for batch in train_loader:\n",
        "        sentence_ids = batch[\"sentence_ids\"].to(device)\n",
        "\n",
        "        outputs = model.generate(input_ids=sentence_ids)\n",
        "        predictions = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
        "\n",
        "        print(\"Predictions:\", predictions)\n",
        "        break  # Show one batch\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hCsydz_TOLQ_"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oRKn9SpyOLOj"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "V28"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "TPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}